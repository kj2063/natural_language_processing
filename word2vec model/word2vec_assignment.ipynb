{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427036eb-9f9f-4351-bea1-eeaac263b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fee274-d2e7-4665-afb3-e9fba4a4c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\powerinfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizer 다운로드\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfd0ca-b25b-4fc3-85ea-288afad3646f",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1812b8c7-0902-438c-b223-0aba939894fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom datasets import load_dataset\\nimport re\\n\\ndef text_regexp_processing(text):\\n    # 1. 괄호 및 괄호 안 내용 제거\\n    text = re.sub(r\\'\\\\([^)]*\\\\)\\', \\'\\', text)\\n\\n    # 2. 축약어 \\' 토큰 생성\\n    text = re.sub(r\"\\'(\\\\w)\", r\"APOSTROPHE\\x01\", text)\\n    \\n    # 3. 마침표(.),문자,공백 을 제외한 특수문자 제거\\n    text = re.sub(r\\'[^a-zA-Z0-9.\\\\s]\\', \\'\\', text)\\n\\n    # 4. 축약어 토큰 -> \\' 로 변경\\n    text = text.replace(\"APOSTROPHE\", \"\\'\")\\n    \\n    return text\\n\\n# 추출할 list 갯수 param\\nlist_num = 20000\\n    \\ndataset = load_dataset(\"cbt\", \"CN\")\\n\\ntexts = dataset[\\'train\\'][\\'sentences\\'][:list_num]\\n\\ncleaned_texts = []\\nfor text in texts:\\n    text = \\' \\'.join(text)\\n\\n    text = text_regexp_processing(text)\\n    \\n    cleaned_texts.append(text)\\n\\nwith open(f\"cbt_clean_{list_num}_train.txt\", \"w\", encoding=\"utf-8\") as f:\\n    for line in cleaned_texts:\\n        f.write(line + \" \")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Children's Book Test text 데이터 추출 => https://huggingface.co/datasets/cam-cst/cbt\n",
    "'''\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def text_regexp_processing(text):\n",
    "    # 1. 괄호 및 괄호 안 내용 제거\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "    # 2. 축약어 ' 토큰 생성\n",
    "    text = re.sub(r\"'(\\w)\", r\"APOSTROPHE\\1\", text)\n",
    "    \n",
    "    # 3. 마침표(.),문자,공백 을 제외한 특수문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z0-9.\\s]', '', text)\n",
    "\n",
    "    # 4. 축약어 토큰 -> ' 로 변경\n",
    "    text = text.replace(\"APOSTROPHE\", \"'\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 추출할 list 갯수 param\n",
    "list_num = 20000\n",
    "    \n",
    "dataset = load_dataset(\"cbt\", \"CN\")\n",
    "\n",
    "texts = dataset['train']['sentences'][:list_num]\n",
    "\n",
    "cleaned_texts = []\n",
    "for text in texts:\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    text = text_regexp_processing(text)\n",
    "    \n",
    "    cleaned_texts.append(text)\n",
    "\n",
    "with open(f\"cbt_clean_{list_num}_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in cleaned_texts:\n",
    "        f.write(line + \" \")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0361c-3769-4dab-a9fc-f72ea78ea364",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e67bd1-7011-4ccd-8043-f62c8262be9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences num: 337054\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 선택\n",
    "# 1. cbt_clean_10000_train.txt => 약 16만 문장\n",
    "# 2. cbt_clean_20000_train.txt => 약 33만 문장\n",
    "# 그 이상의 데이터는 위에서 추출 가능\n",
    "selected_train_text = 'cbt_clean_20000_train.txt'\n",
    "\n",
    "with io.open(selected_train_text, encoding='utf-8') as fin:\n",
    "    texts = fin.read()\n",
    "\n",
    "# Tokenize the text\n",
    "sentences = nltk.sent_tokenize(texts)\n",
    "\n",
    "print(f\"sentences num: {len(sentences)}\")\n",
    "\n",
    "tokenized_texts = [list(map(str.lower, nltk.word_tokenize(sentence))) for sentence in sentences]\n",
    "\n",
    "# word2Vec 모델 학습\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=200,   # 임베딩 차원 수\n",
    "    window=5,          # 컨텍스트 윈도우 크기\n",
    "    min_count=5,       # 최소 단어 빈도 수\n",
    "    workers=multiprocessing.cpu_count(),  # 병렬 처리 스레드 수\n",
    "    sg=1,              # 1: Skip-gram, 0: CBOW\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b1fd6-b1f6-4d2f-88ce-f59a9754fa35",
   "metadata": {},
   "source": [
    "## Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8552fca2-b497-4d13-b089-4754d12210ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSimilarity(word):\n",
    "    sims = model.wv.most_similar(word, topn=10)\n",
    "    return print(f'[{word}] : {sims}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e0e4c0a-fdfe-4627-99f3-5101b36c738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[queen] : [('king', 0.4102681279182434), ('miranda', 0.4016765058040619), ('sheik', 0.3872550427913666), ('bitterness', 0.36715617775917053), ('conquest', 0.35870882868766785), ('mistveiled', 0.3586718440055847), ('displeases', 0.3546091318130493), ('approval', 0.3479524552822113), ('lowest', 0.34773555397987366), ('fanned', 0.3437991738319397)]\n",
      "\n",
      "[she] : [('he', 0.7055046558380127), ('her', 0.7024463415145874), ('they', 0.5571861267089844), ('it', 0.5416773557662964), ('but', 0.5342990159988403), ('and', 0.5315732359886169), ('to', 0.5082858204841614), ('that', 0.501986026763916), ('as', 0.4880739152431488), ('.', 0.4869805872440338)]\n",
      "\n",
      "[portrait] : [('painters', 0.41107723116874695), ('admirer', 0.3869825601577759), ('packet', 0.38291996717453003), ('snowflakes', 0.3622840344905853), ('bravery', 0.3588334023952484), ('coping', 0.34431248903274536), ('horserace', 0.33865538239479065), ('clearsighted', 0.3348265290260315), ('tinkled', 0.33473753929138184), ('pretense', 0.33381035923957825)]\n",
      "\n",
      "[boy] : [('girl', 0.4399719834327698), ('tiidu', 0.36502790451049805), ('85', 0.3599376380443573), ('woman', 0.3529011607170105), ('tenderhearted', 0.3471296727657318), ('piteously', 0.34594959020614624), ('tanoki', 0.3450629413127899), ('prince', 0.34399640560150146), ('princess', 0.3438763916492462), ('horseload', 0.34313079714775085)]\n",
      "\n",
      "[royal] : [('highness', 0.5053191781044006), ('academy', 0.47117218375205994), ('vestments', 0.4675137400627136), ('guesthouse', 0.4646722972393036), ('stationers', 0.4616761803627014), ('banner', 0.4580838978290558), ('herald', 0.4533427655696869), ('forecourt', 0.4383678436279297), ('installed', 0.43485766649246216), ('ancestress', 0.43054649233818054)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printSimilarity('queen')\n",
    "printSimilarity('she')\n",
    "printSimilarity('portrait')\n",
    "printSimilarity('boy')\n",
    "printSimilarity('royal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
